{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Ideas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ROUGH DRAFT ALL CODE IS IN PROGRESS MY WORKSTYLE IS SLOPPY I KNOW**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#includes\n",
    "import math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import librosa\n",
    "import sys\n",
    "from scipy import signal\n",
    "import scipy.io.wavfile as wav\n",
    "from scipy.fft import fft, ifft\n",
    "import contextlib\n",
    "import os\n",
    "import soundfile as sf\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio\n",
    "### Voice recording phase: the system records voice at 5-s intervals.\n",
    "\n",
    "### Recorded data are bandpass filtered (50 Hz~3000 Hz)\n",
    "\n",
    "### Data are filtered with a Wiener filter. \n",
    "The Wiener filter minimizes the Mean Square Error (MSE) between the estimated random process and the desired operation. This filter is generally used to remove noise from a recorded voice.\n",
    "### Short sounds and background noise are removed.\n",
    " First, an adaptive threshold to remove background noise has been used. The reference level of environmental noise must be calculated. As the noise in the disaster area is high and highly variable, an adaptive background noise reference has been defined according to the equation:\n",
    "$$Ref_{noise}  = αVol_t + (1 − α)Vol_{t-1}$$\n",
    "where α is the smoothing factor of Refnoise change, Volt is the average volume [dB] of current 5 s voice data, Volt−1 is the volume of previous 5 s voice data. It has been empirically found that a = 30% yields the best performance. Then, if the volume of the sound sample is lower than 1.3 times Refnoise, the algorithm identifies the sound sample as environmental noise and discards it. Sound signals that are 1.3 times higher than Refnoise are suspect sounds. Then, the algorithm checks the length of this suspect sound. As human voice sound is assumed to last more than 300 ms, sounds shorter than 300 ms are removed. After removing short sounds, this suspect sound is processed with SVM to identify possible human noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "#Band-Pass\n",
    "\n",
    "def bandpass(wavname):\n",
    "        sample_rate, wav_file = wav.read(wavname)\n",
    "\n",
    "        low = 50\n",
    "        high = 3000\n",
    "\n",
    "        nyq = 0.5*sample_rate\n",
    "        lowpass = low/nyq\n",
    "        highpass = high/nyq\n",
    "        b, a = signal.butter(3, [lowpass, highpass], btype=\"band\")\n",
    "        print(b)\n",
    "        print(wav_file)\n",
    "        filtered_wav = signal.filtfilt(b, a, wav_file, padtype=None)\n",
    "        print(filtered_wav)\n",
    "\n",
    "        wav.write(str(wavname+\"_filtered.wav\"), sample_rate, filtered_wav.astype(\"float32\"))\n",
    "        return str(wavname+\"_filtered.wav\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiener_filter(wavdata):\n",
    "    return signal.wiener(wavdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def short_sound_removal(wavdata, pwa_volume, smoothing_factor):\n",
    "    # pwa_volume -- average volume of the previous 5sec clip.\n",
    "    # smoothing factor -- float between 0 and 1, 0.3 is empirically tested to be best\n",
    "    volume = sum(wavdata)/len(wavdata)\n",
    "    refnoise = smoothing_factor*volume - (1-smoothing_factor)\n",
    "    \n",
    "    removed_ambient = [0 if x < refnoise else x for x in wavdata]\n",
    "\n",
    "    # Then scan for segments greater than 300ms of nonzeros\n",
    "    # PENDING, DIVIDING CLIPS BY LENGTH IS GONNA BE A LITTLE WEIRD I THINK\n",
    "    final_wav = 1 # replace once figured out\n",
    "    return final_wav\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Segmentation.\n",
    " The 5-s audio signal, after removing short sounds and background noise, is broken into shorter audio samples of 10 ms.\n",
    "### Audio statistical features are computed for these shorter 10-ms audio samples.\n",
    "### SVM Classification.\n",
    " Sounds are differentiated in human voice or noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "unpack requires a buffer of 3840 bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\terra\\OneDrive\\Desktop\\Classes\\CAPSTONE SHIT\\AlgoPrep.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/terra/OneDrive/Desktop/Classes/CAPSTONE%20SHIT/AlgoPrep.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/terra/OneDrive/Desktop/Classes/CAPSTONE%20SHIT/AlgoPrep.ipynb#W3sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Split the frames into channels and write the clip to a new WAV file\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/terra/OneDrive/Desktop/Classes/CAPSTONE%20SHIT/AlgoPrep.ipynb#W3sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m clip_frames_array \u001b[39m=\u001b[39m wave\u001b[39m.\u001b[39;49mstruct\u001b[39m.\u001b[39;49munpack(\u001b[39m\"\u001b[39;49m\u001b[39m%d\u001b[39;49;00m\u001b[39mh\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m%\u001b[39;49m (clip_frames \u001b[39m*\u001b[39;49m num_channels), clip_frames_raw)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/terra/OneDrive/Desktop/Classes/CAPSTONE%20SHIT/AlgoPrep.ipynb#W3sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m clip_frames_channels \u001b[39m=\u001b[39m [clip_frames_array[ch::num_channels] \u001b[39mfor\u001b[39;00m ch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_channels)]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/terra/OneDrive/Desktop/Classes/CAPSTONE%20SHIT/AlgoPrep.ipynb#W3sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m clip_file \u001b[39m=\u001b[39m wave\u001b[39m.\u001b[39mopen(\u001b[39m'\u001b[39m\u001b[39moutput/clip_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.wav\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(clip_num), \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31merror\u001b[0m: unpack requires a buffer of 3840 bytes"
     ]
    }
   ],
   "source": [
    "def splitter(wavname):\n",
    "    # read\n",
    "    sample_rate, wavfile = wav.read(wavname)\n",
    "    # if length is not exactly 5 seconds, crop from both ends until so, or pad.\n",
    "    \n",
    "    # Set the length of each clip in milliseconds\n",
    "    clip_length = 10\n",
    "    clip_frames = int(clip_length * sample_rate / 500)\n",
    "\n",
    "    \n",
    "    # Create a directory to store the output clips\n",
    "    # Not sure if we will want this, but depending on whether we use an SVM or something else may want this temp stored somewhere for\n",
    "    # other applications.\n",
    "    if not os.path.exists('output'):\n",
    "        os.makedirs('output')\n",
    "\n",
    "    # Read and process each clip from the WAV file\n",
    "    clip_num = 0\n",
    "    while clip_num:\n",
    "        # Read the next clip from the WAV file\n",
    "        clip_frames_raw = wavfile.readframes(clip_frames * num_channels)\n",
    "        if not clip_frames_raw:\n",
    "            break\n",
    "        clipped_frames.append(clip_frames_raw)\n",
    "\n",
    "        # Split the frames into channels and write the clip to a new WAV file\n",
    "        # clip_frames_array = wavfile.struct.unpack(\"%dh\" % (clip_frames * num_channels), clip_frames_raw)\n",
    "        # clip_frames_channels = [clip_frames_array[ch::num_channels] for ch in range(num_channels)]\n",
    "        wav.write('output/clip_{}.wav'.format(clip_num), sample_rate)\n",
    "        # Increment the clip counter\n",
    "        clip_num += 1\n",
    "\n",
    "    return clipped_frames\n",
    "    ## Currently writes all the clips to a separate directory, can change if we want to handle everything in one program\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy Entropy\n",
    "\n",
    "#### $$H(X) = −\\Sigma^{n}_{i=1}p(x_i)\\log{p(x_i)}$$\n",
    "\n",
    "where X is a discrete RV with pdf $p(x)$ and readings $\\{x_i\\}_{i=1}^n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(hist): \n",
    "    # Calculate the entropy of the histogram\n",
    "    return -np.sum(hist * np.log2(hist + 1e-6)) #Safeguard value to prevent log of 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal Energy\n",
    "\n",
    "#### $$E_s = \\int^{\\infty}_{-\\infty}|x(t)|^2dt$$\n",
    "\n",
    "Where $x(t)$ is a continuous-time signal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## May need to look into this one more -- Not sure if applying correctly\n",
    "## Basically square all values, sum them up, divide by amount of values for a discrete integral instead of continuous\n",
    "def compute_signal_energy(histogram):\n",
    "    signal_energy = np.sum(np.square(histogram))/len(histogram)\n",
    "    return signal_energy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Crossing Rate\n",
    "\n",
    "#### $$ZCR = \\frac{1}{T-1}\\Sigma^{T-1}_{t=1}1_{R>0}(s_ts_{t-1})$$\n",
    "\n",
    "where $s$ is a voice signal of length $T$ and $1_{R>0}$ is an indicator function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_zero_cross(clip):\n",
    "    # Set indicator function values of main hist\n",
    "    signage = np.sign(clip)\n",
    "    #Find sum\n",
    "    total = np.sum(np.abs(np.diff(signage)))\n",
    "    return total/((len(clip)-1))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Roll-Off\n",
    "\n",
    "#### $$\\Sigma^{R_t}_{n=1}M_t[n] = 0.85 \\times \\Sigma^{N}_{n=1}M_t[n]$$\n",
    "\n",
    "Where $M_t[n]$ is the magnitude of the Fourier Transform at frame $t$ and frequnecy bin $n$ and $R_t$ is the frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spectral_rolloff(fourier, sample_rate):\n",
    "    return librosa.feature.spectral_rolloff(y=fourier, sr=sample_rate, n_fft=64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Centroid\n",
    "\n",
    "#### $$C = \\frac{\\Sigma^{N-1}_{n=0}f(n)x(n)}{\\Sigma^{N-1}_{n=0}x(n)}$$\n",
    "\n",
    "where $x(n)$ represents the weighted frequency value, or magnitude, of bin $n$, and $f(n)$ represents the center frequency of that bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spectral_centroid(fourier, sample_rate):\n",
    "    return librosa.feature.spectral_rolloff(y=fourier, sr=sample_rate, n_fft=64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Flux\n",
    "\n",
    "#### $$F_t = \\Sigma^{N}_{n=1}(N_t[n]-N_{t-1}[n])^2$$\n",
    "\n",
    "where $N_t[n]$ and $N_{t-1}[n]$ are the normalized Fourier transform at frames t and t-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spectral_flux(n_fourier):\n",
    "    #TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then, a driver that gathers all the information for the clips\n",
    "\n",
    "For each 5s clip, divide into 1000 5ms clips.\n",
    "Need to discuss whether we are evaluating 1 5s clip or 1000 5ms clips for analysis.  I vote 1 5s clip with 6 attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(wavname):\n",
    "    #filter the audio bandpass\n",
    "    bandpass_wav_file = bandpass(wavname)\n",
    "    #Wiener filter\n",
    "    wiener = wiener_filter(bandpass_wav_file)\n",
    "    #Remove short sounds\n",
    "    \n",
    "    #Preprocessing of original audio complete, begin splitting and feature definition.\n",
    "    splits = splitter(filtered_wav_file)\n",
    "    rate, data = wav.read(filtered_wav_file)\n",
    "    fourier = fft(data)\n",
    "    min_fourier = min(fourier)\n",
    "    max_fourier = max(fourier)\n",
    "    normalized_fourier = [(x-min_fourier/(max_fourier-min_fourier)) for x in fourier]\n",
    "\n",
    "    features = np.array() #stores for each clip\n",
    "    # Call codee above for splitting wav data into clips\n",
    "    i=0\n",
    "    for clip in clips:\n",
    "        # Calculate the decibel levels for the clip\n",
    "        db = librosa.amplitude_to_db(np.abs(clip), ref=np.max)\n",
    "        # Calculate the histogram of the decibel levels\n",
    "        hist, bins = np.histogram(db, bins='auto', density=True)\n",
    "        #Entropy - can be performed on histogram\n",
    "        features[i][0] = compute_entropy(hist)\n",
    "        #Signal Energy - can be performed on histogram\n",
    "        features[i][1] = compute_signal_energy(hist)\n",
    "        #Zero Crossing Rate - performed on audio\n",
    "        features[i][2] = compute_zero_cross(hist)\n",
    "        #Spectral Roll Off\n",
    "        features[i][3] = compute_spectral_rolloff(fourier)\n",
    "        #Spectral Centroid\n",
    "        features[i][4] = compute_spectral_centroid(fourier)\n",
    "        #Spectral Flux\n",
    "        features[i][5] = compute_spectral_flux(normalized_fourier)\n",
    "    \n",
    "    # For entropy, ZCT, Roll Off, Centroid, compute SD\n",
    "    \n",
    "    # For Signal Energy, Flux, compute SD by Squared Mean\n",
    "    \n",
    "\n",
    "    #Features prepped for SVM\n",
    "    return collected_data\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classification\n",
    "Going to have to train an SVM somehow.  I think the best way to do it would be to record a long wav on our current mic setup, split it, mark which sections had a voice and which ones did not (i.e. set it up in the living room for a bit).  Then listen and manually track voices.  After that, use as training data.  If we are measuring 5 second clips, we are going to want perhaps 30 minutes of volume input.  Can also look for an SVM model online that is built or some voice data, I'm sure it exists out there.  But the model might work better if it was built on the audio from our mic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build SVM Model\n",
    "\n",
    "# For REFNOISE -- Take ambient, compute average, use that for threshold of previous 5 second clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run snippet through SVM Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $CO_2$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIDAR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thermal Camera (?)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful Links\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5af5fef448d746d87dd7e11f6cafdd37f3997a1001242f3a5af0c63ff779b4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
