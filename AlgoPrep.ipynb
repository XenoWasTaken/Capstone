{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Ideas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ROUGH DRAFT ALL CODE IS IN PROGRESS MY WORKSTYLE IS SLOPPY I KNOW**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#includes\n",
    "import math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import librosa\n",
    "import sys\n",
    "from scipy import signal\n",
    "import scipy.io.wavfile as wav\n",
    "from scipy.fft import fft, ifft\n",
    "import contextlib\n",
    "import os\n",
    "import soundfile as sf\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio\n",
    "### Voice recording phase: the system records voice at 5-s intervals.\n",
    "\n",
    "### Recorded data are bandpass filtered (50 Hz~3000 Hz)\n",
    "\n",
    "### Data are filtered with a Wiener filter. \n",
    "The Wiener filter minimizes the Mean Square Error (MSE) between the estimated random process and the desired operation. This filter is generally used to remove noise from a recorded voice.\n",
    "### Short sounds and background noise are removed.\n",
    " First, an adaptive threshold to remove background noise has been used. The reference level of environmental noise must be calculated. As the noise in the disaster area is high and highly variable, an adaptive background noise reference has been defined according to the equation:\n",
    "$$Ref_{noise}  = αVol_t + (1 − α)Vol_{t-1}$$\n",
    "where α is the smoothing factor of Refnoise change, Volt is the average volume [dB] of current 5 s voice data, Volt−1 is the volume of previous 5 s voice data. It has been empirically found that a = 30% yields the best performance. Then, if the volume of the sound sample is lower than 1.3 times Refnoise, the algorithm identifies the sound sample as environmental noise and discards it. Sound signals that are 1.3 times higher than Refnoise are suspect sounds. Then, the algorithm checks the length of this suspect sound. As human voice sound is assumed to last more than 300 ms, sounds shorter than 300 ms are removed. After removing short sounds, this suspect sound is processed with SVM to identify possible human noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "#Band-Pass\n",
    "\n",
    "def bandpass(wavdata, sample_rate):\n",
    "\n",
    "        low = 50\n",
    "        high = 3000\n",
    "\n",
    "        nyq = 0.5*sample_rate\n",
    "        lowpass = low/nyq\n",
    "        highpass = high/nyq\n",
    "        b, a = signal.butter(3, [lowpass, highpass], btype=\"band\")\n",
    "        filtered_wav = signal.filtfilt(b, a, wavdata, padtype=None)\n",
    "\n",
    "        # wav.write(str(wavname+\"_filtered.wav\"), sample_rate, filtered_wav.astype(\"float32\"))\n",
    "        return filtered_wav\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiener_filter(wavdata):\n",
    "    return signal.wiener(wavdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def short_sound_removal(wavdata, pwa_volume, smoothing_factor, sample_rate):\n",
    "    # pwa_volume -- average volume of the previous 5sec clip.\n",
    "    # smoothing factor -- float between 0 and 1, 0.3 is empirically tested to be best\n",
    "    volume = sum(wavdata)/len(wavdata)\n",
    "    refnoise = smoothing_factor*volume - (1-smoothing_factor)*pwa_volume\n",
    "    # Then scan for segments greater than 300ms of nonzeros\n",
    "    final_sounds = get_long_sounds(wavdata, refnoise, 0.3, sample_rate)\n",
    "    return final_sounds\n",
    "\n",
    "\n",
    "def get_long_sounds(samples, threshold, min_duration, sample_rate):\n",
    "    # Find all non-ambient samples\n",
    "    non_ambient = np.where(samples > threshold, samples, 0)\n",
    "    longer_than = min_duration*sample_rate\n",
    "    found = False\n",
    "    start = 0\n",
    "    long_sounds = list()\n",
    "    for i in range(len(non_ambient)):\n",
    "        #New start\n",
    "        if (non_ambient[i] > 0) and (not found):\n",
    "            start = i\n",
    "            found = True\n",
    "        #Have started, found a 0 after threshold\n",
    "        elif (non_ambient[i]<0) and (i-start >= longer_than):\n",
    "            found = False\n",
    "            #record the sound\n",
    "            long_sounds.append(np.array(non_ambient[start:i-1]))\n",
    "        #Have started, found a 0 before threshold\n",
    "        elif (non_ambient[i]<0) and (i-start < longer_than):\n",
    "            found = False\n",
    "    \n",
    "    return long_sounds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Segmentation.\n",
    " The 5-s audio signal, after removing short sounds and background noise, is broken into shorter audio samples of 10 ms.\n",
    "### Audio statistical features are computed for these shorter 10-ms audio samples.\n",
    "### SVM Classification.\n",
    " Sounds are differentiated in human voice or noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter(wavdata, sample_rate):\n",
    "    # read\n",
    "    # Define the clip length in seconds\n",
    "    clip_length = 0.01\n",
    "\n",
    "    # Calculate the number of clips\n",
    "    num_clips = int(np.ceil(len(wavdata) / (sample_rate * clip_length)))\n",
    "\n",
    "    # Create an empty list to store the clips\n",
    "    clips = []\n",
    "\n",
    "    # Loop over the clips\n",
    "    for i in range(num_clips):\n",
    "        # Calculate the start and end samples of the clip\n",
    "        start_sample = int(i * sample_rate * clip_length)\n",
    "        end_sample = min(int((i + 1) * sample_rate * clip_length), len(wavdata))\n",
    "\n",
    "        # Extract the clip from the data\n",
    "        clip = wavdata[start_sample:end_sample]\n",
    "\n",
    "        # Append the clip to the list\n",
    "        clips.append(clip)\n",
    "    return clips\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test splitter\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy Entropy\n",
    "\n",
    "#### $$H(X) = −\\Sigma^{n}_{i=1}p(x_i)\\log{p(x_i)}$$\n",
    "\n",
    "where X is a discrete RV with pdf $p(x)$ and readings $\\{x_i\\}_{i=1}^n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(hist): \n",
    "    # Calculate the entropy of the histogram\n",
    "    return -np.sum(hist * np.log2(hist + 1e-6)) #Safeguard value to prevent log of 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal Energy\n",
    "\n",
    "#### $$E_s = \\int^{\\infty}_{-\\infty}|x(t)|^2dt$$\n",
    "\n",
    "Where $x(t)$ is a continuous-time signal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "## May need to look into this one more -- Not sure if applying correctly\n",
    "## Basically square all values, sum them up, divide by amount of values for a discrete integral instead of continuous\n",
    "def compute_signal_energy(histogram):\n",
    "    signal_energy = np.sum(np.square(histogram))/len(histogram)\n",
    "    return signal_energy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Crossing Rate\n",
    "\n",
    "#### $$ZCR = \\frac{1}{T-1}\\Sigma^{T-1}_{t=1}1_{R>0}(s_ts_{t-1})$$\n",
    "\n",
    "where $s$ is a voice signal of length $T$ and $1_{R>0}$ is an indicator function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_zero_cross(clip):\n",
    "    # Set indicator function values of main hist\n",
    "    signage = np.sign(clip)\n",
    "    #Find sum\n",
    "    total = np.sum(np.abs(np.diff(signage)))\n",
    "    return total/((len(clip)-1))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Roll-Off\n",
    "\n",
    "#### $$\\Sigma^{R_t}_{n=1}M_t[n] = 0.85 \\times \\Sigma^{N}_{n=1}M_t[n]$$\n",
    "\n",
    "Where $M_t[n]$ is the magnitude of the Fourier Transform at frame $t$ and frequnecy bin $n$ and $R_t$ is the frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spectral_rolloff(fourier, sample_rate):\n",
    "    import librosa\n",
    "\n",
    "    # calculate the power spectrum\n",
    "    magnitude = np.abs(fourier) ** 2\n",
    "\n",
    "    #accumulate\n",
    "    cumulative_sum = np.cumsum(magnitude, axis=0)\n",
    "\n",
    "    # find the frequency bin index at which 85% of the power is contained\n",
    "    total_power = np.sum(magnitude)\n",
    "    cutoff_index = np.argmax(cumulative_sum > 0.85 * total_power)\n",
    "\n",
    "    # calculate the spectral rolloff frequency as the frequency bin center\n",
    "    # corresponding to the cutoff index\n",
    "    spectral_rolloff = librosa.core.fft_frequencies(sr=sample_rate, n_fft=fourier.shape[0])[cutoff_index]\n",
    "\n",
    "    # print the spectral rolloff frequency in Hz\n",
    "    return spectral_rolloff\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Centroid\n",
    "\n",
    "#### $$C = \\frac{\\Sigma^{N-1}_{n=0}f(n)x(n)}{\\Sigma^{N-1}_{n=0}x(n)}$$\n",
    "\n",
    "where $x(n)$ represents the weighted frequency value, or magnitude, of bin $n$, and $f(n)$ represents the center frequency of that bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spectral_centroid(clip, sample_rate):\n",
    "    magnitudes = np.abs(np.fft.rfft(clip))  # Compute the magnitude spectrum\n",
    "    freqs = np.fft.rfftfreq(len(clip), d=1/sample_rate)  # Compute the frequency bins\n",
    "    return np.sum(magnitudes * freqs) / np.sum(magnitudes)  # Compute the weighted average of frequency bins"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Flux\n",
    "\n",
    "#### $$F_t = \\Sigma^{N}_{n=1}(N_t[n]-N_{t-1}[n])^2$$\n",
    "\n",
    "where $N_t[n]$ and $N_{t-1}[n]$ are the normalized Fourier transform at frames t and t-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spectral_flux(fourier_curr, fourier_prev):\n",
    "    # compute magnitude spectrum for each clip\n",
    "    magnitude1 = np.abs(fourier_curr)\n",
    "    magnitude2 = np.abs(fourier_prev)\n",
    "\n",
    "    # compute spectral flux between the two clips\n",
    "    spectral_flux = np.sum(np.abs(magnitude2 - magnitude1))\n",
    "\n",
    "    # print the spectral flux value\n",
    "    return spectral_flux\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then, a driver that gathers all the information for the clips\n",
    "\n",
    "For each 5s clip, divide into 1000 5ms clips.\n",
    "Need to discuss whether we are evaluating 1 5s clip or 1000 5ms clips for analysis.  I vote 1 5s clip with 6 attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(wavname, ref_noise):\n",
    "    sample_rate, wavdata = wav.read(wavname)\n",
    "    #filter the audio bandpass\n",
    "    bandpass_wav = bandpass(wavdata, sample_rate)\n",
    "    #Wiener filter\n",
    "    wiener = wiener_filter(bandpass_wav)\n",
    "    #Remove short sounds\n",
    "    candidates = short_sound_removal(wiener, ref_noise, 0.3, sample_rate)\n",
    "    \n",
    "    features = list() #stores for each clip\n",
    "    #Preprocessing of original audio complete, begin splitting and feature definition.\n",
    "    for candidate in candidates:\n",
    "        #split into 10ms sections\n",
    "        splits = splitter(candidate, sample_rate)\n",
    "        #take fourier of first clip for flux purposes\n",
    "        prev_fourier = librosa.stft(splits[0])\n",
    "        min_prev = min(prev_fourier)\n",
    "        max_prev = max(prev_fourier)\n",
    "        prev_fourier = [(x-min_prev/(max_prev-min_prev)) for x in prev_fourier]\n",
    "        #start at second clip for flux purposes\n",
    "        for clip in splits[1:]:\n",
    "            fourier = librosa.stft(clip)\n",
    "            min_fourier = min(fourier)\n",
    "            max_fourier = max(fourier)\n",
    "            normalized_fourier = [(x-min_fourier/(max_fourier-min_fourier)) for x in fourier]\n",
    "            # Calculate the decibel levels for the clip\n",
    "            db = librosa.amplitude_to_db(np.abs(clip), ref=np.max)\n",
    "            # Calculate the histogram of the decibel levels\n",
    "            hist, bins = np.histogram(db, bins='auto', density=True)\n",
    "            features.append(np.array([compute_entropy(hist), compute_signal_energy(hist), compute_zero_cross(hist), compute_spectral_rolloff(fourier, sample_rate), compute_spectral_centroid(fourier, sample_rate), compute_spectral_flux(normalized_fourier, prev_fourier)]))\n",
    "\n",
    "            #save for next clips flux\n",
    "            prev_fourier = normalized_fourier\n",
    "\n",
    "    # For entropy, ZCT, Roll Off, Centroid, compute SD\n",
    "    std_entropy = np.std([clip_features[0] for clip_features in features])\n",
    "    std_ZCT = np.std([clip_features[2] for clip_features in features])\n",
    "    std_rolloff = np.std([clip_features[3] for clip_features in features])\n",
    "    std_centroid = np.std([clip_features[4] for clip_features in features])\n",
    "    # For Signal Energy, Flux, compute SD by Squared Mean\n",
    "    # Not sure about the difference between these two, ask Prof Murphy maybe? For now, just use regular std\n",
    "    std_sig_nrg = np.std([clip_features[1] for clip_features in features])\n",
    "    std_flux = np.std([clip_features[5] for clip_features in features])\n",
    "    #Features prepped for SVM\n",
    "    return (std_entropy, std_sig_nrg, std_ZCT, std_rolloff, std_centroid, std_flux)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\terra\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\librosa\\core\\spectrum.py:256: UserWarning: n_fft=2048 is too large for input signal of length=480\n",
      "  warnings.warn(\n",
      "C:\\Users\\terra\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\numpy\\fft\\_pocketfft.py:70: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  r = pfi.execute(a, is_real, is_forward, fct)\n",
      "C:\\Users\\terra\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\librosa\\core\\spectrum.py:256: UserWarning: n_fft=2048 is too large for input signal of length=161\n",
      "  warnings.warn(\n",
      "C:\\Users\\terra\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\librosa\\core\\spectrum.py:256: UserWarning: n_fft=2048 is too large for input signal of length=212\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.20082477178883112, 0.0004529460258841722, 0.09173359953046255, 499.085310037785, 2.9381627519454868e-09, 870468.7725904444)\n"
     ]
    }
   ],
   "source": [
    "#Testdrive\n",
    "print(collect_data(\"input1_mono.wav\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classification\n",
    "Going to have to train an SVM somehow.  I think the best way to do it would be to record a long wav on our current mic setup, split it, mark which sections had a voice and which ones did not (i.e. set it up in the living room for a bit).  Then listen and manually track voices.  After that, use as training data.  If we are measuring 5 second clips, we are going to want perhaps 30 minutes of volume input.  Can also look for an SVM model online that is built or some voice data, I'm sure it exists out there.  But the model might work better if it was built on the audio from our mic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build SVM Model\n",
    "\n",
    "# For REFNOISE -- Take ambient, compute average, use that for threshold of previous 5 second clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run snippet through SVM Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $CO_2$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIDAR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thermal Camera (?)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful Links\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5af5fef448d746d87dd7e11f6cafdd37f3997a1001242f3a5af0c63ff779b4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
